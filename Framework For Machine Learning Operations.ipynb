{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework for Building Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The generic MLOps workflow brings together Data Engineering, DevOps and Machine Learning\n",
    "- It is generally composed of the MLOps pipeline and drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *MLOps Pipeline*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The MLOps pipeline performs operations including building, deploying and monitoring models.\n",
    "- All models trained, deployed, and\n",
    "monitored using the MLOps method are end-to-end traceable and their lineage is logged in\n",
    "order to trace the origins of the model, which includes the source code the model used to\n",
    "train, the data used to train and test the model, and parameters used to converge the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Drivers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The key drivers for the MLOps pipeline include data, code, artifacts, middleware and infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**\n",
    "- To manage data in ML applications, data is handled in these steps: data acquisition, data annotation, data cataloging, data preparation, data quality checking, data sampling, and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code**\n",
    "- There are three essential modules of code that drive the MLOps pipeline:\n",
    "training code, testing code, and application code. \n",
    "- These scripts or code are executed using the CI/CD and data pipelines to ensure the robust working of the MLOps pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Artifacts**\n",
    "- The MLOps pipeline generates artifacts such as data, serialized models,\n",
    "code snippets, system logs, ML model training, and testing metrics information. \n",
    "- All these artifacts are useful for the successful working of the MLOps pipeline, ensuring its traceability and sustainability. \n",
    "- These artifacts are managed using middleware services such as the model registry, workspaces, logging services, source code management services, databases, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Middleware**\n",
    "- Middleware refers to computer software that provides services to software applications that are more than those available from the OS.\n",
    "- Middleware services ensure multiple applications to automate and orchestrate\n",
    "processes for the MLOps pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Infrastructure**\n",
    "- Infrastructure essentially reers to storage and computing resources to ensure the successful working of the MLOps pipeline.\n",
    "- When it comes to the infrastructure, there are various options such as on-premises resources or infrastructure as a service (IaaS), which is cloud\n",
    "services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A fully automated MLOps workflow can be achieved through the ptimization and synergy of the drivers with the MLOps pipeline.\n",
    "- An advantage of having an automated MLOps workflow is the increase in the efficiency of the IT team by reducing the time spent working on repeatable tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterizing you Machine Learning Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning solution development process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While ML offers many possibilities to augment and automate business, in order to get the best out of ML teams involved in ML-driven buisness transformation it is important to understand both ML and the business itself, including aspects such as value-chain analyis, use-case identification and business simulations to validate transformation.\n",
    "- Understanding the business is the first step of ML solutions, followed by data analysis where data is acquired, versioned and stores, after which it is consumed for ML modeling using data pipelines where feature engineering is done to get the right features to train the model. \n",
    "We evaluate the trained models and package them for deployment. Deployment and monitoring are done using a pipeline taking advantage of Continuous Integration/Continuous Deployment\n",
    "(CI/CD) features that enable real-time and continuous deployment to serve trained ML\n",
    "models to the users. \n",
    "- This process ensures robust and scalable ML solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Models**\n",
    "- Supervised learning\n",
    "- Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hybrid models**<br>\n",
    "\n",
    "*Semi-supervised learning*\n",
    "- Some data is labled and large amounts of data are unlabeled.<br>\n",
    "\n",
    "*Self supervised learning*\n",
    "- Different from unsupervised learning in that it does not focus on clustering and grouping.<br>\n",
    "\n",
    "*Multi-instance learning*\n",
    "- Supervised learning where data is not labeled by individual samples but rather in categories and samples.<br>\n",
    "\n",
    "*Multitask learning*\n",
    "- Model trained on one dataset then used to solve multiple tasks, eg using word embeddings in NLP.<br>\n",
    "\n",
    "*Reinforcement learning*\n",
    "- An agent, such as a robot system, learns to operate in a defined environment to perform sequential decision-making tasks or achieve a pre-defined goal. \n",
    "- Simultaneously, the agent learns based on continuously evaluated feedback and rewards from the environment.<br>\n",
    "\n",
    "*Ensemble learning*\n",
    "- Two or more models trained on the same data and the result is the average of the outputs of the various models used to determine the final prediction.<br>\n",
    "\n",
    "*Transfer learning*\n",
    "- Model is trained to perform a task, nd is transfered to another model to act as a starting point for finetuning or trainin for performing another task, eg, pretrained model like BERT models.<br>\n",
    "\n",
    "*Federated learning*\n",
    "- ML done is a collaborative fashion, training process distributed accross devices and data isn't shared for privacy and security."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical models**<br>\n",
    "*Inducive learning*\n",
    "- It involves a process of learning by example, where a system tries to generalize a general function or rule from a set of observed instances. \n",
    "- For example, when we fit an ML model, it is a process of induction.<br>\n",
    "\n",
    "*Deductive learning*\n",
    "- Reverse of induction, using general rules to determine specific outcomes. \n",
    "- Induction goes from the specific to the general, deduction goes from the general to the specific.<br>\n",
    "\n",
    "*Transductive learning*\n",
    "- Specific or similar data samples from the training data are compared to reason about or predict an outcome. \n",
    "- For example, in the case of the k-nearest neighbors algorithm, it uses specific data samples on which to base its outcome rather than\n",
    "generalizing the outcome or modeling with the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HITL Models**\n",
    "- In HITL(Human-in-the-loop) models, human-machine interaction enables the algorithm to mimic human-like behaviors and outcomes.\n",
    "- Human involvement is a key driver in these models, and humans validate, label and train the model to maintain its accuracy.<br>\n",
    "\n",
    "*Human-centered reinforcement learning*\n",
    "- Also known as interactive reinorcement learning and is a hybrid of reinforcement learning as human are involved in the process of monitoring the agent's learning and providing feedback to shape the learning process.\n",
    "- Based on the feedback received from the task environment and human expert, the agent augments its behavior and actions.<br>\n",
    "\n",
    "*Active learning*\n",
    "- In active learning a trained model asks the HITL(human user) questions during the inference process to clear up incertitude during learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuring your MLOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementing efficient MLOps in a business results in increased effciency, ig performance and great collaboration that is repeatable and traceable within the orgaization.\n",
    "- MLOps can be categorized according to ML operations, data scale, size of team, infrastructure and tools used as well as the business model being worked on.\n",
    "- The categories are:<br>\n",
    "\n",
    "*1. Small data ops*\n",
    "- ML models are created by a small team of data scientists for narrow and well-defined problems.\n",
    "- ML models are created in computers and left or deployed on the cloud for inference.\n",
    "- In this case pitfalls may include lack of a streamlined CI/CD approach to deploy models.<br>\n",
    "\n",
    "*2. Big data ops*\n",
    "- Involves teams of data scientists and engineers working on large-scale big data processes to perform ML training or inference. \n",
    "- ML models are developed by data scientists and the model deployment is done by data/software engineers.\n",
    "- They use big data tools eg spark to build and orchestrate data pipelines and high powered processors like GPUs and TPUs to speed up data processing and ML training.\n",
    "- Most focus is given to building models and little to monitoring them.\n",
    "\n",
    "*3. Hybrid MLOps*\n",
    "- Involves teams of data scientists, data engineers and DevOps engineers.\n",
    "- They work on well-defined problems by implementing robust and scalable software engineering solutions.\n",
    "\n",
    "*4. Large-scale MLOps*\n",
    "- Common in big ompanies with medium or large sized engineering teams composed of data scientists, data engineers and DevOps engineers.\n",
    "- Characterized by large-scale inference operations and big data operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once the business and tech requirements of the ML operations have been characterized, an implementation roadmap is put in place to ensure smooth development and implementation of a robust and scalable MLOps solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation roadmap for MLOps solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a generic implementation roadmap that can facilitate MLOps for any ML problem in detail. The goal is usually to solve problems with the right solutions.<br>\n",
    "\n",
    "*Phase 1 - ML development*\n",
    "Infrastructure Setup\n",
    "- Configure and setup development and test environments.\n",
    "- Ensure necessary infrastructure(compute, storage, software tools) is provided for training and deployment of ML models\n",
    "\n",
    "ML Development\n",
    "- Developing ML models within efficient framework that enables automation and optimization.\n",
    "- Building and managing data pipelines.\n",
    "- Testing model performance.\n",
    "\n",
    "*Phase 2 - Transition to operations*\n",
    "Pre-requisites\n",
    "- Model artifacts with necessary logging and audibility to track model performance and functionality.\n",
    "- Model is tested for inference and functionality and documented.\n",
    "\n",
    "Key tasks\n",
    "- Serializing and containerizing of model artifacts.\n",
    "- Model serving\n",
    "- Deployments of models to production environment using CI/CD and acceptance testing.\n",
    "- Compliance with quality assurance guidelines.\n",
    "\n",
    "*Phase 3 - Operations*\n",
    "- Performance of deployed models is monitored in terms of model drift, bias etc.\n",
    "- Based on performance continual learning is enabled through model retraining and alerts and actions are enabled.\n",
    "- Simultaneously, we monitor logs in telemetry data for the production\n",
    "environment to detect any possible errors and resolve them on the go to ensure the uninterrupted working of the production system. We also manage data pipelines, the ML platform, and security on the go. \n",
    "- With the successful implementation of this phase, we can monitor the deployed models and retrain them in a robust, scalable, and secure manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcd2c53e9736f599d911168c349fa93ea4da0fc59fb270cea2da0241c3d0904b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
